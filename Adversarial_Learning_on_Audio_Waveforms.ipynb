{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1Zz3vkNCwVP"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip -q install torch torchaudio --upgrade\n",
        "!pip -q install matplotlib ipywidgets\n",
        "\n",
        "import os, math, random, numpy as np, torch, torchaudio\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchaudio.datasets import SPEECHCOMMANDS\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "seed = 42\n",
        "random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "print(\"âœ… Ready on\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "DATA_ROOT = \"./data\"\n",
        "TARGET_LABELS = [\"yes\", \"no\", \"up\", \"down\"]  # keep small for speed\n",
        "SAMPLE_RATE = 16000\n",
        "DURATION = 1.0  # seconds\n",
        "NUM_SAMPLES = int(SAMPLE_RATE * DURATION)\n",
        "\n",
        "class SubsetSC(SPEECHCOMMANDS):\n",
        "    def __init__(self, subset: str = None):\n",
        "        super().__init__(DATA_ROOT, download=True)\n",
        "        def load_list(filename):\n",
        "            filepath = os.path.join(self._path, filename)\n",
        "            with open(filepath) as f: return [os.path.normpath(os.path.join(self._path, ln.strip())) for ln in f]\n",
        "        if subset == \"validation\":\n",
        "            self._walker = load_list(\"validation_list.txt\")\n",
        "        elif subset == \"testing\":\n",
        "            self._walker = load_list(\"testing_list.txt\")\n",
        "        elif subset == \"training\":\n",
        "            excludes = set(load_list(\"validation_list.txt\") + load_list(\"testing_list.txt\"))\n",
        "            self._walker = [w for w in self._walker if w not in excludes]\n",
        "\n",
        "    def __getitem__(self, n):\n",
        "        item = super().__getitem__(n)  # (waveform, sample_rate, label, speaker_id, utterance_number)\n",
        "        waveform, sr, label, *_ = item\n",
        "        # keep only target labels; map others to 'other' or skip\n",
        "        if label not in TARGET_LABELS:\n",
        "            return None\n",
        "        return (waveform, sr, label)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    batch = [b for b in batch if b is not None]\n",
        "    if len(batch) == 0: return None\n",
        "    waves, srs, labels = zip(*batch)\n",
        "    # pad/crop to NUM_SAMPLES and resample if needed\n",
        "    out = []\n",
        "    for w, sr in zip(waves, srs):\n",
        "        if sr != SAMPLE_RATE:\n",
        "            w = torchaudio.functional.resample(w, sr, SAMPLE_RATE)\n",
        "        if w.shape[1] < NUM_SAMPLES:\n",
        "            pad = NUM_SAMPLES - w.shape[1]\n",
        "            w = F.pad(w, (0, pad))\n",
        "        else:\n",
        "            w = w[:, :NUM_SAMPLES]\n",
        "        out.append(w)\n",
        "    x = torch.stack(out, dim=0)          # [B, 1, T]\n",
        "    y = torch.tensor([TARGET_LABELS.index(l) for l in labels], dtype=torch.long)\n",
        "    return x, y\n",
        "\n",
        "train_set = SubsetSC(\"training\")\n",
        "val_set   = SubsetSC(\"validation\")\n",
        "test_set  = SubsetSC(\"testing\")\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader   = DataLoader(val_set, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader  = DataLoader(test_set, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "print(\"Batches:\", len(train_loader), len(val_loader), len(test_loader))\n"
      ],
      "metadata": {
        "id": "U8BbwOA7G-QR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ðŸ§  CNN audio classifier (log-mel inside forward)\n",
        "N_MELS = 64\n",
        "\n",
        "class AudioCNN(nn.Module):\n",
        "    def __init__(self, n_classes=len(TARGET_LABELS)):\n",
        "        super().__init__()\n",
        "        self.melspec = torchaudio.transforms.MelSpectrogram(\n",
        "            sample_rate=SAMPLE_RATE, n_fft=1024, hop_length=256, n_mels=N_MELS\n",
        "        )\n",
        "        self.amplog = torchaudio.transforms.AmplitudeToDB(stype='power')\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, padding=1), nn.BatchNorm2d(16), nn.ReLU(), nn.MaxPool2d((2,2)),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d((2,2)),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.AdaptiveMaxPool2d((8,8)),\n",
        "        )\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64*8*8, 128), nn.ReLU(), nn.Dropout(0.2),\n",
        "            nn.Linear(128, n_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, waveform):  # waveform: [B,1,T]\n",
        "        with torch.no_grad():  # Mel layers are differentiable; set no_grad=False if you want to backprop through them\n",
        "            m = self.melspec(waveform)       # [B,1, n_mels, time]\n",
        "            m = self.amplog(m)\n",
        "        z = self.net(m)\n",
        "        return self.head(z)\n",
        "\n",
        "model = AudioCNN().to(device)\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "g-BJf4eQHEJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ðŸš‚ Train for a few epochs\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def run_epoch(loader, train=True):\n",
        "    model.train(train)\n",
        "    total, correct, loss_sum = 0, 0, 0.0\n",
        "    for batch in loader:\n",
        "        if batch is None: continue\n",
        "        x, y = batch\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        if train:\n",
        "            optimizer.zero_grad()\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        loss_sum += loss.item() * x.size(0)\n",
        "        pred = logits.argmax(dim=1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += x.size(0)\n",
        "    return loss_sum/total, correct/total\n",
        "\n",
        "EPOCHS = 3\n",
        "for ep in range(1, EPOCHS+1):\n",
        "    tr_loss, tr_acc = run_epoch(train_loader, True)\n",
        "    va_loss, va_acc = run_epoch(val_loader, False)\n",
        "    print(f\"Epoch {ep}: train loss {tr_loss:.4f} acc {tr_acc:.3f} | val loss {va_loss:.4f} acc {va_acc:.3f}\")\n"
      ],
      "metadata": {
        "id": "lo7498PfHGyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_loss, test_acc = run_epoch(test_loader, False)\n",
        "print(f\"Clean Test: loss {test_loss:.4f} | acc {test_acc:.3f}\")\n"
      ],
      "metadata": {
        "id": "VcOD50YGHMdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def fgsm_attack(model, waveforms, labels, epsilon=0.002):\n",
        "    # waveforms: [B,1,T], requires_grad for FGSM\n",
        "    wave = waveforms.clone().detach().to(device)\n",
        "    wave.requires_grad_(True)\n",
        "\n",
        "    logits = model(wave)\n",
        "    loss = F.cross_entropy(logits, labels.to(device))\n",
        "    model.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # sign of gradient\n",
        "    grad_sign = wave.grad.data.sign()\n",
        "    adv_wave = wave + epsilon * grad_sign\n",
        "    adv_wave = torch.clamp(adv_wave, -1.0, 1.0).detach()\n",
        "    return adv_wave\n",
        "\n",
        "def eval_under_attack(model, loader, epsilon=0.002, max_batches=30):\n",
        "    model.eval()\n",
        "    total, correct = 0, 0\n",
        "    for b_idx, batch in enumerate(loader):\n",
        "        if b_idx >= max_batches: break\n",
        "        if batch is None: continue\n",
        "        x, y = batch\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        adv = fgsm_attack(model, x, y, epsilon=epsilon)\n",
        "        with torch.no_grad():\n",
        "            logits = model(adv)\n",
        "            pred = logits.argmax(1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += x.size(0)\n",
        "    return correct/total\n",
        "\n",
        "for eps in [0.0, 0.0005, 0.001, 0.002, 0.004]:\n",
        "    if eps == 0.0:\n",
        "        acc = eval_under_attack(model, test_loader, epsilon=0.0)  # forwards only\n",
        "    else:\n",
        "        acc = eval_under_attack(model, test_loader, epsilon=eps)\n",
        "    print(f\"Epsilon {eps:.4f} â†’ accuracy {acc:.3f}\")\n"
      ],
      "metadata": {
        "id": "LH8OBaepHPuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ðŸŽ§ Inspect one sample (audio + spectrograms)\n",
        "def show_mel(wave, sr=SAMPLE_RATE, title=\"Mel\"):\n",
        "    melspec = torchaudio.transforms.MelSpectrogram(sample_rate=sr, n_fft=1024, hop_length=256, n_mels=64)(wave)\n",
        "    mlog = torchaudio.transforms.AmplitudeToDB(stype='power')(melspec)\n",
        "    plt.imshow(mlog.squeeze(0).cpu().numpy(), origin='lower', aspect='auto')\n",
        "    plt.title(title); plt.xlabel(\"Frames\"); plt.ylabel(\"Mel bins\"); plt.colorbar(); plt.show()\n",
        "\n",
        "batch = next(iter(test_loader))\n",
        "x, y = batch\n",
        "x0 = x[0:1].to(device)\n",
        "y0 = y[0:1].to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    pred_clean = model(x0).argmax(1).item()\n",
        "\n",
        "x_adv = fgsm_attack(model, x0, y0, epsilon=0.002)\n",
        "with torch.no_grad():\n",
        "    pred_adv = model(x_adv).argmax(1).item()\n",
        "\n",
        "print(\"True label:\", TARGET_LABELS[y0.item()])\n",
        "print(\"Pred (clean):\", TARGET_LABELS[pred_clean], \" | Pred (FGSM):\", TARGET_LABELS[pred_adv])\n",
        "\n",
        "# waveforms to CPU numpy\n",
        "w_clean = x0[0,0].detach().cpu().numpy()\n",
        "w_adv   = x_adv[0,0].detach().cpu().numpy()\n",
        "\n",
        "# Play audio (might be subtle; headphones recommended)\n",
        "display(Audio(w_clean, rate=SAMPLE_RATE))\n",
        "display(Audio(w_adv,   rate=SAMPLE_RATE))\n",
        "\n",
        "# Visualize mel\n",
        "plt.figure(figsize=(10,3)); plt.plot(w_clean); plt.title(\"Waveform (clean)\"); plt.show()\n",
        "plt.figure(figsize=(10,3)); plt.plot(w_adv);   plt.title(\"Waveform (adversarial)\"); plt.show()\n",
        "show_mel(x0[0].cpu())\n",
        "show_mel(x_adv[0].cpu(), title=\"Mel (adversarial)\")\n"
      ],
      "metadata": {
        "id": "qp6aHAtpHUan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8z0Z6DCzHVG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nNbewmebHQVP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}